{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de Ativação e suas respectivas derivadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(X, func='sigmoid', derivative=False):\n",
    "    \n",
    "    if func == 'relu':\n",
    "        \n",
    "        f_x = np.where(f_x <= 0, 0, 1) if derivative else np.maximum(0, X)\n",
    "        \n",
    "    elif func == 'tanh':\n",
    "        \n",
    "        f_x = (1 - (X ** 2)) if derivative else np.tanh(X)\n",
    "        \n",
    "    elif func == 'sigmoid':\n",
    "        \n",
    "        f_x = X * (1 - X) if derivative else 1/(1 + np.exp(-X))\n",
    "    \n",
    "    return f_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialização dos pesos e bias da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera os pesos e bias para a inicializacao da rede\n",
    "def init_layers(nn_architecture, seed = 1, debug = False):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    nn_params_values = {}\n",
    "    \n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "\n",
    "        layer_idx = idx + 1\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "\n",
    "        W_name = 'W' + str(layer_idx)\n",
    "        b_name = 'b' + str(layer_idx)\n",
    "        #nn_params_values[W_name] = np.random.randn(layer_output_size, layer_input_size) * 0.01\n",
    "        #nn_params_values[b_name] = np.random.randn(layer_output_size, 1) * 0.01\n",
    "        nn_params_values[W_name] = np.random.uniform(size=(layer_output_size, layer_input_size))\n",
    "        nn_params_values[b_name] = np.random.uniform(size=(layer_output_size, 1))\n",
    "\n",
    "        if (debug):\n",
    "            print(f\"{W_name}.shape: {nn_params_values[W_name].shape}\")\n",
    "            print(f\"{b_name}.shape: {nn_params_values[b_name].shape}\")\n",
    "            print('-')\n",
    "        \n",
    "    return nn_params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_dim': 120, 'output_dim': 84, 'activation': 'relu'}, {'input_dim': 84, 'output_dim': 10, 'activation': 'softmax'}]\n"
     ]
    }
   ],
   "source": [
    "# definindo as camadas da rede\n",
    "nn_architecture = [\n",
    "    {\"input_dim\": 120, \"output_dim\": 84, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 84, \"output_dim\": 10, \"activation\": \"softmax\"}\n",
    "]\n",
    "print(nn_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape: (84, 120)\n",
      "b1.shape: (84, 1)\n",
      "-\n",
      "W2.shape: (10, 84)\n",
      "b2.shape: (10, 1)\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "# gera os pesos e bias das camadas da rede aleatoriamente\n",
    "nn_params_values = init_layers(nn_architecture, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passagem \"para frente\" da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_forward(X, nn_params_values, nn_architecture, debug=False):\n",
    "    \n",
    "    if (debug):\n",
    "        print(f\"X.shape: {X.shape}\")\n",
    "        print('-')\n",
    "    \n",
    "    # Guarda a passagem em cache para ser utilizado no backpropagation\n",
    "    cache = {}\n",
    "    \n",
    "    A_curr = X\n",
    "    cache[\"A0\"] = A_curr\n",
    "    \n",
    "    # itera sobre as camadas da rede\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        \n",
    "        # nomeamos as camadas a partir de 1\n",
    "        layer_idx = idx + 1\n",
    "        \n",
    "        # valores de entrada passam a ser a saida da camada anterior\n",
    "        A_prev = A_curr\n",
    "        if (debug):\n",
    "            print(f'A{str(idx)}.shape: {A_prev.shape}')\n",
    "        \n",
    "        # funcao de ativacao a ser executada na camada atual\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        # pesos da camada atual\n",
    "        W_curr = nn_params_values[\"W\" + str(layer_idx)]\n",
    "        \n",
    "        # bias da camada atual\n",
    "        b_curr = nn_params_values[\"b\" + str(layer_idx)]\n",
    "        \n",
    "        # executa uma passagem na camada atual\n",
    "        Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "        A_curr = activation_function(Z_curr, func=activ_function_curr, derivative=False)\n",
    "        \n",
    "        # guarda a passagem em cache\n",
    "        cache[\"W\" + str(layer_idx)] = W_curr\n",
    "        cache[\"b\" + str(layer_idx)] = b_curr\n",
    "        cache[\"Z\" + str(layer_idx)] = Z_curr\n",
    "        cache[\"A\" + str(layer_idx)] = A_curr\n",
    "    \n",
    "    cache[\"A\" + str(len(nn_architecture))] = A_curr\n",
    "    if (debug):\n",
    "        print(f'A{len(nn_architecture)}.shape: {A_curr.shape}')\n",
    "        print('-')\n",
    "    \n",
    "    # retorna a saida da ultima camada da rede e um cache com os valores de cada camada\n",
    "    return A_curr, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodos para auxiliar o debug\n",
    "\n",
    "def print_cache(cache, nn_architecture):\n",
    "    \n",
    "    print('------------ cache ------------')\n",
    "    print(f\"X/A0: \\n{cache['A0']}\")\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "    \n",
    "        print('-')\n",
    "        print(f\"W{layer_idx}: \\n{cache['W' + str(layer_idx)]}\")\n",
    "        print(f\"b{layer_idx}: \\n{cache['b' + str(layer_idx)]}\")\n",
    "        print(f\"Z{layer_idx}: \\n{cache['Z' + str(layer_idx)]}\")\n",
    "        print(f\"A{layer_idx}: \\n{cache['A' + str(layer_idx)]}\")\n",
    "        print(f\"Activation Function: {layer['activation']}\")\n",
    "    print('-------------------------------')\n",
    "\n",
    "def print_params(nn_params_values, nn_architecture):\n",
    "    \n",
    "    print('------------ params ------------')\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        print('-')\n",
    "        print(f\"W{layer_idx}: \\n{nn_params_values['W' + str(layer_idx)]}\")\n",
    "        print(f\"b{layer_idx}: \\n{nn_params_values['b' + str(layer_idx)]}\")\n",
    "    print('--------------------------------')\n",
    "              \n",
    "def print_delta(delta_values, nn_architecture):\n",
    "    \n",
    "    print('------------ delta ------------')\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        print('-')\n",
    "        print(f\"dZ{layer_idx}: \\n{delta_values['dZ' + str(layer_idx)]}\")\n",
    "        print(f\"dW{layer_idx}: \\n{delta_values['dW' + str(layer_idx)]}\")\n",
    "        print(f\"db{layer_idx}: \\n{delta_values['db' + str(layer_idx)]}\")\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução de uma passagem na rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn_architecture: \n",
      "[{'input_dim': 2, 'output_dim': 1, 'activation': 'sigmoid'}, {'input_dim': 1, 'output_dim': 1, 'activation': 'sigmoid'}]\n",
      "\n",
      "X.shape: (2, 4) \n",
      "[[0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "\n",
      "y.shape: (1, 4) \n",
      "[[0 1 1 0]]\n",
      "\n",
      "y_hat: (1, 4) \n",
      "[[ 0.57393661  0.58665059  0.58151636  0.59283222]]\n",
      "\n",
      "------------ cache ------------\n",
      "X/A0: \n",
      "[[0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "-\n",
      "W1: \n",
      "[[ 0.417022    0.72032449]]\n",
      "b1: \n",
      "[[ 0.00011437]]\n",
      "Z1: \n",
      "[[  1.14374817e-04   7.20438868e-01   4.17136380e-01   1.13746087e+00]]\n",
      "A1: \n",
      "[[ 0.50002859  0.67270365  0.60279781  0.75721315]]\n",
      "Activation Function: sigmoid\n",
      "-\n",
      "W2: \n",
      "[[ 0.30233257]]\n",
      "b2: \n",
      "[[ 0.14675589]]\n",
      "Z2: \n",
      "[[ 0.29793082  0.35013612  0.3290013   0.37568609]]\n",
      "A2: \n",
      "[[ 0.57393661  0.58665059  0.58151636  0.59283222]]\n",
      "Activation Function: sigmoid\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "nn_architecture = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "    {\"input_dim\": 1, \"output_dim\": 1, \"activation\": \"sigmoid\"}\n",
    "]\n",
    "nn_params_values = init_layers(nn_architecture)\n",
    "print(f'nn_architecture: \\n{nn_architecture}')\n",
    "print()\n",
    "    \n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]]).T\n",
    "print(f'X.shape: {X.shape} \\n{X}')\n",
    "print()\n",
    "\n",
    "y = np.array([[0, \n",
    "               1, \n",
    "               1, \n",
    "               0]])\n",
    "print(f\"y.shape: {y.shape} \\n{y}\")\n",
    "\n",
    "y_hat, cache = nn_forward(X, nn_params_values, nn_architecture)\n",
    "print()\n",
    "print(f\"y_hat: {y_hat.shape} \\n{y_hat}\")\n",
    "print()\n",
    "print_cache(cache, nn_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de custo/perda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_error(Y, Y_hat):\n",
    "    \"\"\"\n",
    "    Calcula função de custo por meio do erro quadrado médio entre as previsões e as amostras de treinamento.\n",
    "    \n",
    "    Arguments:\n",
    "    Y -- Conjunto de amostras de treinamento.\n",
    "    Y_hat -- Previsões realizadas pela rede.\n",
    "        \n",
    "    Returns:\n",
    "    mse -- Valor escalar indicando o quão distante as previsões foram das amostras de treinamento.\n",
    "    \"\"\"\n",
    "    mse = np.square(np.subtract(Y, Y_hat)).mean()\n",
    "    return mse\n",
    "\n",
    "\n",
    "def cross_entropy(Y, Y_hat):\n",
    "    \"\"\"\n",
    "    Calcula a função de custo por meio da entropia cruzada.\n",
    "\n",
    "    Arguments:\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "    Y_hat -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(Y_hat)) + np.multiply(1 - Y, np.log(1 - Y_hat)))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # garante q o valor retornado sera escalar\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atualização dos pessos (backpropagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_backward_propagation(y_hat, y, cache, nn_architecture):\n",
    "\n",
    "    dZ = y_hat - y \n",
    "    delta_values = {}\n",
    "    \n",
    "    for layer_idx, layer in reversed(list(enumerate(nn_architecture, len(nn_architecture)-1))):\n",
    "        \n",
    "        # rede de uma unica camada\n",
    "        if (len(nn_architecture) == 1):\n",
    "            layer_idx = 1\n",
    "        \n",
    "        A_prev = cache[\"A\" + str(layer_idx-1)]\n",
    "        m = A_prev.shape[1]\n",
    "        \n",
    "        A_curr = cache[\"A\" + str(layer_idx)]\n",
    "        Z_curr = cache[\"Z\" + str(layer_idx)]\n",
    "        W_curr = cache[\"W\" + str(layer_idx)]\n",
    "        b_curr = cache[\"b\" + str(layer_idx)]\n",
    "             \n",
    "        # matriz de pesos\n",
    "        dW = np.dot(dZ, A_prev.T) * (1 / m)\n",
    "        \n",
    "        # bias\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) * (1 / m)\n",
    "        \n",
    "        # valores de ajuste a ser descontado dos pesos\n",
    "        delta_values[\"dZ\" + str(layer_idx)] = dZ\n",
    "        delta_values[\"dW\" + str(layer_idx)] = dW\n",
    "        delta_values[\"db\" + str(layer_idx)] = db\n",
    "        \n",
    "        # funcao de ativacao\n",
    "        dZ = np.dot(W_curr.T, dZ) * activation_function(A_prev, func='sigmoid', derivative=True)\n",
    "    \n",
    "    return delta_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_update_parameters(nn_params_values, delta_values, nn_architecture, learning_rate):\n",
    "\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        \n",
    "        nn_params_values[\"W\" + str(layer_idx)] -= learning_rate * delta_values[\"dW\" + str(layer_idx)]        \n",
    "        nn_params_values[\"b\" + str(layer_idx)] -= learning_rate * delta_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return nn_params_values;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prob_into_class(y_hat):\n",
    "    probs_ = np.copy(y_hat)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_value(Y, Y_hat):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, nn_architecture, epochs, learning_rate):\n",
    "    \n",
    "    nn_params_values = init_layers(nn_architecture)\n",
    "    print()\n",
    "    print_params(nn_params_values, nn_architecture)\n",
    "    \n",
    "    print(f\"\\nTreinando com taxa de aprendizado = {learning_rate} e {epochs} épocas:\")\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        y_hat, cache = nn_forward(X, nn_params_values, nn_architecture)\n",
    "        \n",
    "        if ((i+1)% 5000) == 0:\n",
    "            #error = cross_entropy(y, y_hat)\n",
    "            error = mean_square_error(y, y_hat)\n",
    "            acuracia = get_accuracy_value(y, y_hat)\n",
    "            print(\"-> Iteração: {:05} - Erro: {:.5f} - Acurácia: {:.5f}\".format((i+1), error, acuracia))\n",
    "            if (error < 0.06):\n",
    "                break\n",
    "        \n",
    "        delta_values = nn_backward_propagation(y_hat, y, cache, nn_architecture)\n",
    "        nn_params_values = nn_update_parameters(nn_params_values, delta_values, nn_architecture, learning_rate)\n",
    "    \n",
    "    return nn_params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn_architecture: \n",
      "[{'input_dim': 2, 'output_dim': 3, 'activation': 'tanh'}, {'input_dim': 3, 'output_dim': 1, 'activation': 'sigmoid'}]\n",
      "\n",
      "X.shape: (2, 4) \n",
      "[[0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "\n",
      "y.shape: (1, 4) \n",
      "[[0 1 1 0]]\n",
      "\n",
      "------------ params ------------\n",
      "-\n",
      "W1: \n",
      "[[  4.17022005e-01   7.20324493e-01]\n",
      " [  1.14374817e-04   3.02332573e-01]\n",
      " [  1.46755891e-01   9.23385948e-02]]\n",
      "b1: \n",
      "[[ 0.18626021]\n",
      " [ 0.34556073]\n",
      " [ 0.39676747]]\n",
      "-\n",
      "W2: \n",
      "[[ 0.53881673  0.41919451  0.6852195 ]]\n",
      "b2: \n",
      "[[ 0.20445225]]\n",
      "--------------------------------\n",
      "\n",
      "Treinando com taxa de aprendizado = 0.01 e 100000 épocas:\n",
      "-> Iteração: 05000 - Erro: 0.22644 - Acurácia: 0.75000\n",
      "-> Iteração: 10000 - Erro: 0.18131 - Acurácia: 0.75000\n",
      "-> Iteração: 15000 - Erro: 0.06075 - Acurácia: 1.00000\n",
      "-> Iteração: 20000 - Erro: 0.01400 - Acurácia: 1.00000\n",
      "\n",
      "y_hat: (1, 4) \n",
      "[[ 0.  1.  1.  0.]]\n",
      "\n",
      "------------ cache ------------\n",
      "X/A0: \n",
      "[[0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "-\n",
      "W1: \n",
      "[[ 2.15406403  2.19923199]\n",
      " [ 6.15093223  6.05570446]\n",
      " [ 0.09240721 -0.26326205]]\n",
      "b1: \n",
      "[[ 0.02537967]\n",
      " [-7.06675931]\n",
      " [ 0.22235046]]\n",
      "Z1: \n",
      "[[ 0.02537967  2.22461166  2.1794437   4.37867569]\n",
      " [-7.06675931 -1.01105485 -0.91582708  5.13987738]\n",
      " [ 0.22235046 -0.04091159  0.31475768  0.05149562]]\n",
      "A1: \n",
      "[[ 0.02537422  0.97689476  0.97473794  0.99968545]\n",
      " [-0.99999854 -0.76619796 -0.72391733  0.99993136]\n",
      " [ 0.21875719 -0.04088878  0.30475912  0.05145015]]\n",
      "Activation Function: tanh\n",
      "-\n",
      "W2: \n",
      "[[ 4.74632681 -2.74882614 -0.16767705]]\n",
      "b2: \n",
      "[[-4.36008083]]\n",
      "Z2: \n",
      "[[-1.5275049   2.38958207  2.20516577 -2.37251147]]\n",
      "A2: \n",
      "[[ 0.17835904  0.91602943  0.90071244  0.085293  ]]\n",
      "Activation Function: sigmoid\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "nn_architecture = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 3, \"activation\": \"tanh\"},\n",
    "    {\"input_dim\": 3, \"output_dim\": 1, \"activation\": \"sigmoid\"}\n",
    "]\n",
    "print(f'nn_architecture: \\n{nn_architecture}')\n",
    "print()\n",
    "\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]]).T\n",
    "print(f'X.shape: {X.shape} \\n{X}')\n",
    "print()\n",
    "\n",
    "y = np.array([[0, \n",
    "               1, \n",
    "               1, \n",
    "               0]])\n",
    "print(f\"y.shape: {y.shape} \\n{y}\")\n",
    "\n",
    "epochs = 100000\n",
    "learning_rate = 0.01\n",
    "nn_params_values = train(X, y, nn_architecture, epochs, learning_rate)\n",
    "\n",
    "y_hat, cache = nn_forward(X, nn_params_values, nn_architecture)\n",
    "print()\n",
    "print(f\"y_hat: {y_hat.shape} \\n{convert_prob_into_class(y_hat)}\")\n",
    "print()\n",
    "print_cache(cache, nn_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução com a rede treinada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (2, 1) \n",
      "[[0 1]]\n",
      "\n",
      "y.shape: (1, 1) \n",
      "[[1]]\n",
      "\n",
      "y_hat: (1, 1) \n",
      "[[ 1.]]\n",
      "\n",
      "------------ cache ------------\n",
      "X/A0: \n",
      "[[0]\n",
      " [1]]\n",
      "-\n",
      "W1: \n",
      "[[ 2.15406403  2.19923199]\n",
      " [ 6.15093223  6.05570446]\n",
      " [ 0.09240721 -0.26326205]]\n",
      "b1: \n",
      "[[ 0.02537967]\n",
      " [-7.06675931]\n",
      " [ 0.22235046]]\n",
      "Z1: \n",
      "[[ 2.22461166]\n",
      " [-1.01105485]\n",
      " [-0.04091159]]\n",
      "A1: \n",
      "[[ 0.97689476]\n",
      " [-0.76619796]\n",
      " [-0.04088878]]\n",
      "Activation Function: tanh\n",
      "-\n",
      "W2: \n",
      "[[ 4.74632681 -2.74882614 -0.16767705]]\n",
      "b2: \n",
      "[[-4.36008083]]\n",
      "Z2: \n",
      "[[ 2.38958207]]\n",
      "A2: \n",
      "[[ 0.91602943]]\n",
      "Activation Function: sigmoid\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 1]]).T\n",
    "print(f'X.shape: {X.shape} \\n{X.T}')\n",
    "print()\n",
    "\n",
    "y = np.array([[1]])\n",
    "print(f'y.shape: {y.shape} \\n{y.T}')\n",
    "print()\n",
    "\n",
    "y_hat, cache = nn_forward(X, nn_params_values, nn_architecture)\n",
    "y_hat_prob = convert_prob_into_class(y_hat)\n",
    "\n",
    "print(f\"y_hat: {y_hat.shape} \\n{y_hat_prob.T}\")\n",
    "print()\n",
    "print_cache(cache, nn_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
